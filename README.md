# TinyLlama 1.1B Chat GGUF (Q6_K)

This repository includes the `tinyllama-1.1b-chat-v1.0.Q6_K.gguf` model file for use with `llama.cpp` or compatible Python libraries.

## Model Info
- Base model: [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- Quantized by: [TheBloke](https://huggingface.co/TheBloke)
- Quantization format: GGUF Q6_K
- License: Apache 2.0
